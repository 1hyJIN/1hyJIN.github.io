<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="实验目的和要求实现MINIST手写数字图像识别分类任务。">
<meta property="og:type" content="article">
<meta property="og:title" content="图像分类及经典CNN实现">
<meta property="og:url" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/index.html">
<meta property="og:site_name" content="HyFan">
<meta property="og:description" content="实验目的和要求实现MINIST手写数字图像识别分类任务。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/idx.png">
<meta property="og:image" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/LetNet.png">
<meta property="og:image" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/AlexNet.png">
<meta property="og:image" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/Res_Block.png">
<meta property="og:image" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/ResNet18.png">
<meta property="og:image" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/VGGNet.jpg">
<meta property="og:image" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/LeNet_Result.png">
<meta property="og:image" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/AlexNet_Result.png">
<meta property="og:image" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/ResNet_Result.png">
<meta property="og:image" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/VGGNet_Result.png">
<meta property="og:image" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/SGD.png">
<meta property="article:published_time" content="2023-05-10T12:42:58.000Z">
<meta property="article:modified_time" content="2023-05-11T16:55:22.053Z">
<meta property="article:author" content="hy">
<meta property="article:tag" content="人工智能">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/idx.png">

<link rel="canonical" href="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>图像分类及经典CNN实现 | HyFan</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">HyFan</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">奔赴山海，保持热爱</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="hy">
      <meta itemprop="description" content="HyFan的个人博客">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="HyFan">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          图像分类及经典CNN实现
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-05-10 20:42:58" itemprop="dateCreated datePublished" datetime="2023-05-10T20:42:58+08:00">2023-05-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-05-12 00:55:22" itemprop="dateModified" datetime="2023-05-12T00:55:22+08:00">2023-05-12</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>11k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>19 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="实验目的和要求"><a href="#实验目的和要求" class="headerlink" title="实验目的和要求"></a>实验目的和要求</h2><p>实现MINIST手写数字图像识别分类任务。</p>
<span id="more"></span>
<p>要求使用多种CNN模型：<br>必选LetNet，AlexNet，ResNet<br>自己再选择至少一个架构实现。</p>
<!--more-->
<h2 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h2><p>Python3.8<br>VsCode</p>
<h2 id="下载MINIST手写数字图像数据以及数据处理"><a href="#下载MINIST手写数字图像数据以及数据处理" class="headerlink" title="下载MINIST手写数字图像数据以及数据处理"></a>下载MINIST手写数字图像数据以及数据处理</h2><h3 id="下载地址：MINIST"><a href="#下载地址：MINIST" class="headerlink" title="下载地址：MINIST"></a>下载地址：<a target="_blank" rel="noopener" href="http://yann.lecun.com/exdb/mnist/">MINIST</a></h3><h3 id="读取数据文件"><a href="#读取数据文件" class="headerlink" title="读取数据文件"></a>读取数据文件</h3><p>下载的文件格式为IDX:</p>
<img src="/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/idx.png" class="" title="idx">

<p>通过导入<code>idx2numpy</code>库，用于将IDX文件格式转化为Numpy数组。</p>
<p>分别从上面四个文件读取训练集图像和标签、测试集图像和标签，并将他们存储在 NumPy 数组中以供进一步处理或分析。</p>
<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>首先需要将图像数据的像素值除以 255.0 来归一化它们。此操作将像素值缩放到 0 和 1 之间，以确保数值稳定性。</p>
<p>接着将图像数据从二维数组（28x28 像素）重塑为包含 784 个特征的一维数组。</p>
<p>同时对标签数据执行单热编码。单热编码是一种用于将分类变量表示为二进制向量的技术。在这种情况下，标签值是表示数字类别的 0 到 9 之间的整数。使用np.eye(10)函数生成一个大小为 10x10 的单位矩阵，其中每一行对应一个唯一的类标签。通过使用标签值对该矩阵进行索引，我们获得了相应的单热编码向量。</p>
<h3 id="划分验证集"><a href="#划分验证集" class="headerlink" title="划分验证集"></a>划分验证集</h3><p>按要求自行划分验证集：</p>
<p>使用<code>train_test_split</code>函数将训练数据分成两部分：实际训练集 <code>(train_images和train_labels)</code> 和验证集 <code>(val_images和val_labels)</code>。设置参数为<code>0.2</code>，即原始训练数据的 <code>20%</code>将用于验证，而其余 <code>80%</code>将用于训练。</p>
<h3 id="创建训练数据集的子集"><a href="#创建训练数据集的子集" class="headerlink" title="创建训练数据集的子集"></a>创建训练数据集的子集</h3><p>数据集比较大，特征数量很多，导致较为复杂的模型在训练时间上很长，于是当使用较复杂一点的模型，我通过选择原始训练数据的一部分进行训练来缩短时间。</p>
<p>因此我创建一个采样器<code>SequentialSampler(train_idx)</code>选择原始训练数据前百分之十的样本进行训练。</p>
<h2 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h2><h3 id="LetNet"><a href="#LetNet" class="headerlink" title="LetNet"></a>LetNet</h3><p>LetNet是深度学习领域的先驱模型之一，在计算机视觉任务的进步中发挥了重要作用，特别是在手写数字识别方面，主要设计用于识别图像中的手写数字，特别是来自 MNIST 数据集。</p>
<p>LetNet网络共有7层，包含卷积层、池化层和全连接层。</p>
<img src="/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/LetNet.png" class="" title="LetNet">

<ul>
<li><p>输入层：LeNet 将灰度图像作为输入，大小为 32x32 像素。</p>
</li>
<li><p>卷积层：LeNet 从两个卷积层开始，每个卷积层后跟一个 sigmoid 激活函数。这些层将一组可学习的过滤器应用于输入图像，执行卷积操作以提取边缘和角落等特征。</p>
</li>
<li><p>池化层：这些层减少了特征图的空间维度，同时保留了重要信息。池化方式统一为平均池化。</p>
</li>
<li><p>全连接层：在卷积层和池化层之后，LeNet 包括两个全连接层。这些层类似于传统的神经网络层，旨在对提取的特征进行分类。第一个全连接层有 120 个节点，第二个全连接层有 84 个节点。两层都使用 sigmoid 激活函数。</p>
</li>
<li><p>输出层：最后一层是一个全连接层，有 10 个节点，代表 10 个可能的类（数字 0 到 9）。输出层使用 softmax 激活函数来生成类的概率分布。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool1 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pool1(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool2(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>AlexNet 是一种深度卷积神经网络 (CNN) 架构，由 Alex Krizhevsky、Ilya Sutskever 和 Geoffrey Hinton 开发。它在 2012 年赢得了 ImageNet 大规模视觉识别挑战赛 (ILSVRC)，显着推进了计算机视觉领域。</p>
<p>AlexNet 包含数百万个标记图像。它由多个卷积层和全连接层组成，采用各种技术来提高性能。以下是其关键组件的概述：</p>
<ul>
<li>输入层：AlexNet 将 RGB 图像作为输入，大小为 224x224 像素。</li>
<li>卷积层：第一个卷积层应用 96 个大小为 11x11 的卷积核，步幅为 4。随后的卷积层使用更小的卷积核尺寸 (3x3) 并具有更小的步幅。卷积核的数量从 96 个增加到 256 个。</li>
<li>最大池化层：在卷积层之间，AlexNet 利用最大池化层来减少空间维度并捕获最显着的特征。池化层的卷积核大小为 3x3，步幅为 2。</li>
<li>全连接层：在卷积层之后，有三个全连接层。第一个全连接层由 4,096 个神经元组成，然后是 ReLU 激活函数和 dropout 正则化以减轻过拟合。接下来的两个全连接层分别有 4,096 和 1,0 个神经元。最后一层使用 softmax 激活来生成 1,0 个类别的概率分布。</li>
<li>Dropout：为了防止过拟合，AlexNet 在第一层和第二层全连接层之后应用了 dropout 正则化。Dropout 在训练期间随机将一部分神经元设置为零，迫使网络学习更稳健的表示。</li>
</ul>
<img src="/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/AlexNet.png" class="" title="AlexNet">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(AlexNet, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Upsample(scale_factor=<span class="number">8</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">96</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(in_features=<span class="number">6400</span>, out_features=<span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            nn.Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            nn.Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br></pre></td></tr></table></figure>

<h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>ResNet是现代计算机视觉模型中一个里程碑式的网络结构，由Kaiming He等人于2016年提出。它旨在通过利用残差连接来解决非常深的神经网络中发生的梯度消失问题。</p>
<p>ResNet 通过引入允许梯度更容易地在网络中流动的跳跃连接来解决训练深度神经网络时会出现梯度消失问题，使网络难以有效学习，阻碍性能的进一步提升的问题。</p>
<p>ResNet中基本构建块是残差块（Residue Block），其基本结构如下图所示。可以看到，相比普通的卷积块，残差块通过加入一个直接连接输入和输出的快速通道来将输入的特征直接叠加到输出特征之上，从而使得输入特征得以保留。</p>
<img src="/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/Res_Block.png" class="" title="Res_Block">

<p>ResNet网络结构主要由残差块和全连接部分组成。</p>
<p>在网络开始的部分采用了和AlexNet相同的 (7 \times 7) 卷积核，并加入了批归一化（Batch Normalization）这一操作，使得网络的特征尺度放缩到同一水平，从而使得网络更容易训练。随后的网络结构由8个残差块组成，其中第一组残差块包含了两个同尺寸连接的普通残差块，之后的三组残差块均有一个半尺寸连接的残差块和一个普通残差块组成。</p>
<p>ResNet 没有在网络末端使用全连接层，而是使用全局平均池化，这减少了模型中的参数数量并有助于防止过度拟合。</p>
<img src="/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/ResNet18.png" class="" title="ResNet18">

<p>对于这个实验，我们只需要实现最少层数的ResNet就可以了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, stride, res_conv=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ResBlock, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=stride),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels)</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">if</span> res_conv:</span><br><span class="line">            self.res_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=<span class="number">1</span>, stride=stride)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.res_conv = <span class="literal">None</span></span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = self.net(x)</span><br><span class="line">        <span class="keyword">if</span> self.res_conv:</span><br><span class="line">            x = self.res_conv(x)</span><br><span class="line">        y = y + x</span><br><span class="line">        <span class="keyword">return</span> self.relu(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Upsample(scale_factor=<span class="number">8</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        self.net.add_module(<span class="string">&#x27;res_1&#x27;</span>, nn.Sequential(</span><br><span class="line">            ResBlock(in_channels=<span class="number">64</span>, out_channels=<span class="number">64</span>, stride=<span class="number">1</span>),</span><br><span class="line">            ResBlock(in_channels=<span class="number">64</span>, out_channels=<span class="number">64</span>, stride=<span class="number">1</span>)</span><br><span class="line">        ))</span><br><span class="line">        self.net.add_module(<span class="string">&#x27;res_2&#x27;</span>, nn.Sequential(</span><br><span class="line">            ResBlock(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>, stride=<span class="number">2</span>, res_conv=<span class="literal">True</span>),</span><br><span class="line">            ResBlock(in_channels=<span class="number">128</span>, out_channels=<span class="number">128</span>, stride=<span class="number">1</span>)</span><br><span class="line">        ))</span><br><span class="line">        self.net.add_module(<span class="string">&#x27;res_3&#x27;</span>, nn.Sequential(</span><br><span class="line">            ResBlock(in_channels=<span class="number">128</span>, out_channels=<span class="number">256</span>, stride=<span class="number">2</span>, res_conv=<span class="literal">True</span>),</span><br><span class="line">            ResBlock(in_channels=<span class="number">256</span>, out_channels=<span class="number">256</span>, stride=<span class="number">1</span>)</span><br><span class="line">        ))</span><br><span class="line">        self.net.add_module(<span class="string">&#x27;res_4&#x27;</span>, nn.Sequential(</span><br><span class="line">            ResBlock(in_channels=<span class="number">256</span>, out_channels=<span class="number">512</span>, stride=<span class="number">2</span>, res_conv=<span class="literal">True</span>),</span><br><span class="line">            ResBlock(in_channels=<span class="number">512</span>, out_channels=<span class="number">512</span>, stride=<span class="number">1</span>)</span><br><span class="line">        ))</span><br><span class="line">        self.net.add_module(<span class="string">&#x27;output&#x27;</span>, nn.Sequential(</span><br><span class="line">            nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(in_features=<span class="number">512</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br></pre></td></tr></table></figure>



<h3 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h3><p>VGGNet由 Karen Simonyan 和 Andrew Zisserman 于 2014 年推出。由于其在图像分类任务中的简单性和有效性而获得了极大的关注和普及。<br>该网络主要包括卷积层和最大池化层。卷积层负责从输入图像中学习空间层次结构，而最大池化层有助于减少空间维度。</p>
<p>VGGNet架构具有统一的配置，更易于理解和实现。卷积层使用小的 3x3 过滤器，步长为 1，填充为 1，这有助于保持空间分辨率。最大池化层有一个 2x2 的窗口，步幅为 2，导致空间维度减半。</p>
<p>VGGNet 的更深层使其能够从图像中捕获更多抽象特征。然而，这种深度架构的缺点是它的大量参数，这使得它的计算量大且内存密集。原始的 VGGNet 模型有大约 1.38 亿个参数。</p>
<p>VGGNet中最为核心的元素即为可变深度的VGG块，VGG块的基本结构是确定的, 只需要指定卷积层的个数以及输入输出的通道数即可。对于每个VGG块的第一个卷积层，我们将输入的通道数匹配为输出的通道数；对于后面的若干卷积层，我们只需要保持输出通道输和输入通道数一致。</p>
<p>这次实验，我们只需实现层数最少的11层VGGNet。与AlexNet类似，在输入核心网络结构前，需要将输入数据放缩到 (224*224) 大小。</p>
<img src="/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/VGGNet.jpg" class="" title="VGGNet">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VGGBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, conv_num, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(VGGBlock, self).__init__()</span><br><span class="line">        self.net = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(conv_num):</span><br><span class="line">            self.net.add_module(</span><br><span class="line">                <span class="string">&quot;conv_&#123;0&#125;&quot;</span>.<span class="built_in">format</span>(i), nn.Sequential(</span><br><span class="line">                    nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                    nn.ReLU()</span><br><span class="line">                )</span><br><span class="line">            )</span><br><span class="line">            in_channels = out_channels</span><br><span class="line">        self.net.add_module(<span class="string">&quot;pool&quot;</span>, nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VGGNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(VGGNet, self).__init__()</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            nn.Upsample(scale_factor=<span class="number">8</span>, mode=<span class="string">&#x27;bilinear&#x27;</span>),</span><br><span class="line">            VGGBlock(<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>),</span><br><span class="line">            VGGBlock(<span class="number">1</span>, <span class="number">64</span>, <span class="number">128</span>),</span><br><span class="line">            VGGBlock(<span class="number">2</span>, <span class="number">128</span>, <span class="number">256</span>),</span><br><span class="line">            VGGBlock(<span class="number">2</span>, <span class="number">256</span>, <span class="number">512</span>),</span><br><span class="line">            VGGBlock(<span class="number">2</span>, <span class="number">512</span>, <span class="number">512</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(in_features=<span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>, out_features=<span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            nn.Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(p=dropout),</span><br><span class="line">            nn.Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<br>
## 实验总结分析

<h3 id="代码实现过程遇到的问题及解决办法"><a href="#代码实现过程遇到的问题及解决办法" class="headerlink" title="代码实现过程遇到的问题及解决办法"></a>代码实现过程遇到的问题及解决办法</h3><p>因为读取数据文件后将数据转化成数组，对于后面的深度学习模型训练不能很好的兼容和格式匹配，因此需要将数据转换为张量，。张量是用于在这些框架中存储和操作数据的基本数据结构。通过将数据转换为张量，确保与框架的兼容性并实现与其他深度学习操作的无缝集成，从而简化数据处理和预处理任务。</p>
<ul>
<li>因此，<br>我用两个张量<code>torch.tensor(train_images,dtype=torch.float).view(-1, 1, 28, 28)</code>和<code>torch.tensor(np.argmax(train_labels, axis=1), dtype=torch.long)</code>创建<code>torch.utils.data.TensorDataset</code>的训练数据集。因为MNIST数据集中的原始图像尺寸为 (1$\times$$28$$\times$28)，所以需要将<code>train_images</code>转化并重塑为维度(-1, 1, 28, 28)。</li>
</ul>
<p>AlexNet和VGGNet框架是对于图像尺寸为（3 * 224 * 224）的，而MINIST数据集原始图像尺寸为 (1 * 28 * 28)的，这一输入在后几层卷积层中甚至无法完成一次卷积操作。</p>
<ul>
<li>因此首先需要将MINIST数据集放缩为（224 * 224）大小。</li>
</ul>
<p>AlexNet的网络结构有着46764746个可训练参数，这一参数量几乎是LeNet的758倍，VGGNet可训练参数的规模甚至达到了超过1亿的级别，相比较AlexNet不到5000万的参数量多了整整一倍。对于如此大量的网络参数，往往需要借助GPU来进行训练和推理，但是因为没有GPU的条件，如果全部数据进行训练需要耗费很长很长时间。</p>
<ul>
<li>因此我创建一个采样器SequentialSampler(train_idx)选择原始训练数据前百分之十的样本进行训练。</li>
</ul>
<h3 id="实验结果对比分析"><a href="#实验结果对比分析" class="headerlink" title="实验结果对比分析"></a>实验结果对比分析</h3><p>训练模型过程我统一使用交叉熵损失作为训练时的损失函数，为了比较各网络结构对分类结果的影响，我通过命令行使用了完全相同的超参数：<br><code>--model lenet --lr 0.001 --dropout 0.0</code><br><code>--model alexnet --lr 0.001 --dropout 0.0</code><br><code>--model resnet --lr 0.001 --dropout 0.0</code><br><code>--model vggnet --lr 0.001 --dropout 0.0</code></p>
<ul>
<li><p>LetNet:</p>
<img src="/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/LeNet_Result.png" class="" title="LeNet_Result">
</li>
<li><p>AlexNet:</p>
<img src="/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/AlexNet_Result.png" class="" title="AlexNet_Result">
</li>
<li><p>ResNet:</p>
<img src="/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/ResNet_Result.png" class="" title="ResNet_Result">
</li>
<li><p>VGGNet:</p>
<img src="/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/VGGNet_Result.png" class="" title="VGGNet_Result">
<br>
可以看到，虽然LeNet的准确率最高，但是AlexNet和ResNet都是训练的部分训练数据而LeNet是全部训练数据。所以总的来说，针对MINISIT数据集，LeNet模型效果已经很好了，ResNet和AlexNet的效果相比，ResNet更好上一点，但是这两类模型在MINIST数据集上训练时间比LeNet要长很多很多，所以对于本次实验所用的这种特征较为简单的数据图像，我更倾向于使用LeNet模型。</li>
</ul>
<h3 id="影响模型性能的原因"><a href="#影响模型性能的原因" class="headerlink" title="影响模型性能的原因"></a>影响模型性能的原因</h3><ul>
<li>学习率:<br>学习率主要是控制模型的学习进度或是速度。高学习率会使损失函数变化加快，但是往往得不到最优解，即在最优解附近来回震荡且震荡幅度较大。但是，使用低学习率可以确保我们不会错过任何局部极小值，同样也意味着我们将花费更长的时间来进行收敛，特别是在被困在高原区域的情况下。<br>在对AlexNet和ResNet模型训练我分别设置了0.01和0.001的学习率，得到了差别很大的结果，0.001的结果在上面可以看到，而0.01的结果只有仅仅 12% 的准确率。因此学习率是影响模型性能的原因之一。  <br></li>
<li>Batchsize:<br>根据随机梯度下降算法原理：<img src="/2023/05/10/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%8F%8A%E7%BB%8F%E5%85%B8CNN%E5%AE%9E%E7%8E%B0/SGD.png" class="" title="SGD">
n(批量大小)也是影响模型性能收敛的重要参数，影响模型的泛化性能。<br></li>
<li>优化算法:<br>优化算法的选择，例如随机梯度下降 (SGD) 或其变体，如 Adam 或 RMSprop，会影响模型收敛和找到最优解的速度。不同的优化算法具有不同的收敛行为，会影响训练速度和最终性能。<br>在本次实验中，我通过分别使用SGD、Adam优化器对各个模型进行训练，结果在不同优化器下，模型训练得到的结果也是大有差异。</li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/05/09/%E6%80%BB%E7%BB%93/" rel="prev" title="总结">
      <i class="fa fa-chevron-left"></i> 总结
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%9B%AE%E7%9A%84%E5%92%8C%E8%A6%81%E6%B1%82"><span class="nav-number">1.</span> <span class="nav-text">实验目的和要求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83"><span class="nav-number">2.</span> <span class="nav-text">实验环境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BDMINIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E6%95%B0%E6%8D%AE%E4%BB%A5%E5%8F%8A%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="nav-number">3.</span> <span class="nav-text">下载MINIST手写数字图像数据以及数据处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80%EF%BC%9AMINIST"><span class="nav-number">3.1.</span> <span class="nav-text">下载地址：MINIST</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6"><span class="nav-number">3.2.</span> <span class="nav-text">读取数据文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="nav-number">3.3.</span> <span class="nav-text">数据处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%92%E5%88%86%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="nav-number">3.4.</span> <span class="nav-text">划分验证集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%AD%90%E9%9B%86"><span class="nav-number">3.5.</span> <span class="nav-text">创建训练数据集的子集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CNN%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.</span> <span class="nav-text">CNN模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LetNet"><span class="nav-number">4.1.</span> <span class="nav-text">LetNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AlexNet"><span class="nav-number">4.2.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ResNet"><span class="nav-number">4.3.</span> <span class="nav-text">ResNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VGGNet"><span class="nav-number">4.4.</span> <span class="nav-text">VGGNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%8A%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95"><span class="nav-number">4.5.</span> <span class="nav-text">代码实现过程遇到的问题及解决办法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90"><span class="nav-number">4.6.</span> <span class="nav-text">实验结果对比分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BD%B1%E5%93%8D%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="nav-number">4.7.</span> <span class="nav-text">影响模型性能的原因</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="hy"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">hy</p>
  <div class="site-description" itemprop="description">HyFan的个人博客</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/1hyJIN" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;1hyJIN" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hy</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">15k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">28 分钟</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>
-->

<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
